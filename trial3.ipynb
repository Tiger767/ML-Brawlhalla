{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML-Brawlhalla: Trial 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "The environment module is not included in this code, so that results cannot be abused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from paiutils import neural_network as nn\n",
    "from paiutils import reinforcement as rl\n",
    "from paiutils import reinforcement_agents as ra\n",
    "\n",
    "from environment import BrawlhallaFollow2 as Brawlhalla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 1\n",
    "time_size = 3\n",
    "env = Brawlhalla(stack_size=stack_size)\n",
    "max_steps = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3, 4107, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 64, 64, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 32)   2432        lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 32)   128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 64)   18496       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 16, 16, 64)   256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 8, 8, 128)    73856       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 8, 8, 128)    512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 4, 4, 256)    295168      batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 4, 4, 256)    1024        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 2, 2, 512)    1180160     batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 2, 2, 512)    2048        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2048)         0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 33)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 2081)         0           flatten[0][0]                    \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 128)          266496      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 128)          512         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 9)            1161        batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,842,249\n",
      "Trainable params: 1,840,009\n",
      "Non-trainable params: 2,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x0 = keras.layers.Input((time_size, 64 * 64 + 11, stack_size))\n",
    "frame_input = keras.layers.Lambda(\n",
    "    lambda x: tf.reshape(tf.transpose(x[:,:,:-11,:], [0, 3, 2, 1]), [-1, 64, 64, stack_size * time_size])\n",
    ")(x0)\n",
    "other_input = keras.layers.Lambda(\n",
    "    lambda x: tf.reshape(x[:,:,-11:,:], [-1, 11 * stack_size * time_size])\n",
    ")(x0)\n",
    "\n",
    "x = nn.conv2d(32, 5, 2)(frame_input)\n",
    "x = nn.conv2d(64, 3, 2)(x)\n",
    "x = nn.conv2d(128, 3, 2)(x)\n",
    "x = nn.conv2d(256, 3, 2)(x)\n",
    "x = nn.conv2d(512, 3, 2)(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Concatenate()([x, other_input])\n",
    "x = nn.dense(128)(x)\n",
    "outputs = keras.layers.Dense(9, activation='softmax')(x)\n",
    "model = keras.models.Model(inputs=x0, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(.0005), loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_memory(shape, dtype):\n",
    "    if shape == (None, time_size, 64 * 64 + 11, stack_size):\n",
    "        void_state = np.append(np.zeros(64 * 64), np.ones(11))\n",
    "        void_state = np.expand_dims(void_state, axis=-1)\n",
    "        return rl.ETDMemory(time_size, void_state)\n",
    "    return rl.Memory()\n",
    "\n",
    "agent = rl.PGAgent(model, .99, create_memory=create_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 12:57:16 - Episode: 1 - Steps: 415 - Total Reward: 0.09500000000001241 - Best Total Reward: 0.09500000000001241 - Average Total Reward: 0.09500000000001241 - Memory Size: 415\n",
      "Time: 12:57:34 - Episode: 2 - Steps: 311 - Total Reward: -0.21099999999999186 - Best Total Reward: 0.09500000000001241 - Average Total Reward: -0.057999999999989726 - Memory Size: 726\n",
      "Time: 12:57:56 - Episode: 3 - Steps: 457 - Total Reward: -1.5269999999999908 - Best Total Reward: 0.09500000000001241 - Average Total Reward: -0.5476666666666568 - Memory Size: 1183\n",
      "Time: 12:58:26 - Episode: 4 - Steps: 677 - Total Reward: 2.9430000000000063 - Best Total Reward: 2.9430000000000063 - Average Total Reward: 0.325000000000009 - Memory Size: 1860\n",
      "Time: 12:58:40 - Episode: 5 - Steps: 277 - Total Reward: -0.8869999999999951 - Best Total Reward: 2.9430000000000063 - Average Total Reward: 0.08260000000000818 - Memory Size: 2137\n",
      "Time: 12:58:50 - Episode: 6 - Steps: 159 - Total Reward: -2.639 - Best Total Reward: 2.9430000000000063 - Average Total Reward: -0.37099999999999317 - Memory Size: 2296\n",
      "Time: 12:59:09 - Episode: 7 - Steps: 363 - Total Reward: 3.9870000000000014 - Best Total Reward: 3.9870000000000014 - Average Total Reward: 0.2515714285714346 - Memory Size: 2659\n",
      "Time: 12:59:17 - Episode: 8 - Steps: 95 - Total Reward: -2.8449999999999998 - Best Total Reward: 3.9870000000000014 - Average Total Reward: -0.13549999999999468 - Memory Size: 2754\n",
      "Time: 12:59:34 - Episode: 9 - Steps: 310 - Total Reward: -3.3999999999999995 - Best Total Reward: 3.9870000000000014 - Average Total Reward: -0.49822222222221746 - Memory Size: 3064\n",
      "Time: 13:00:09 - Episode: 10 - Steps: 859 - Total Reward: 7.551000000000149 - Best Total Reward: 7.551000000000149 - Average Total Reward: 0.3067000000000192 - Memory Size: 3923\n",
      "Time: 13:00:30 - Episode: 11 - Steps: 457 - Total Reward: 5.193000000000046 - Best Total Reward: 7.551000000000149 - Average Total Reward: 0.7509090909091125 - Memory Size: 4380\n",
      "Time: 13:00:55 - Episode: 12 - Steps: 559 - Total Reward: -1.3189999999999937 - Best Total Reward: 7.551000000000149 - Average Total Reward: 0.578416666666687 - Memory Size: 4939\n",
      "Time: 13:01:24 - Episode: 13 - Steps: 609 - Total Reward: 0.9810000000000185 - Best Total Reward: 7.551000000000149 - Average Total Reward: 0.6093846153846356 - Memory Size: 5548\n",
      "Time: 13:01:32 - Episode: 14 - Steps: 103 - Total Reward: -2.843 - Best Total Reward: 7.551000000000149 - Average Total Reward: 0.36278571428573303 - Memory Size: 5651\n",
      "Time: 13:01:39 - Episode: 15 - Steps: 62 - Total Reward: -2.832 - Best Total Reward: 7.551000000000149 - Average Total Reward: 0.14980000000001753 - Memory Size: 5713\n",
      "Time: 13:01:46 - Episode: 16 - Steps: 66 - Total Reward: 4.154000000000001 - Best Total Reward: 7.551000000000149 - Average Total Reward: 0.4000625000000165 - Memory Size: 5779\n",
      "Time: 13:02:00 - Episode: 17 - Steps: 286 - Total Reward: -2.265999999999999 - Best Total Reward: 7.551000000000149 - Average Total Reward: 0.24323529411766262 - Memory Size: 6065\n",
      "Time: 13:02:37 - Episode: 18 - Steps: 845 - Total Reward: -0.47499999999998854 - Best Total Reward: 7.551000000000149 - Average Total Reward: 0.20333333333334866 - Memory Size: 6910\n",
      "Time: 13:03:08 - Episode: 19 - Steps: 724 - Total Reward: 15.226000000000194 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.9940000000000249 - Memory Size: 7634\n",
      "Time: 13:03:23 - Episode: 20 - Steps: 278 - Total Reward: -1.1979999999999975 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.8844000000000237 - Memory Size: 7912\n",
      "Time: 13:03:33 - Episode: 21 - Steps: 139 - Total Reward: -2.1689999999999996 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.7390000000000226 - Memory Size: 8051\n",
      "Time: 13:04:01 - Episode: 22 - Steps: 608 - Total Reward: 2.4419999999999997 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.8164090909091125 - Memory Size: 8659\n",
      "Time: 13:04:12 - Episode: 23 - Steps: 184 - Total Reward: -0.5639999999999956 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.7563913043478468 - Memory Size: 8843\n",
      "Time: 13:04:35 - Episode: 24 - Steps: 530 - Total Reward: 6.61000000000001 - Best Total Reward: 15.226000000000194 - Average Total Reward: 1.000291666666687 - Memory Size: 9373\n",
      "Time: 13:04:45 - Episode: 25 - Steps: 87 - Total Reward: -2.8169999999999997 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.8476000000000196 - Memory Size: 9460\n",
      "Time: 13:04:54 - Episode: 26 - Steps: 105 - Total Reward: -2.2649999999999997 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.7278846153846341 - Memory Size: 9565\n",
      "Time: 13:05:08 - Episode: 27 - Steps: 263 - Total Reward: -2.4129999999999994 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.6115555555555736 - Memory Size: 9828\n",
      "Time: 13:05:23 - Episode: 28 - Steps: 245 - Total Reward: -2.9749999999999996 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.4834642857143031 - Memory Size: 10073\n",
      "Time: 13:05:32 - Episode: 29 - Steps: 101 - Total Reward: -2.731 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.37262068965518924 - Memory Size: 10174\n",
      "Time: 13:05:56 - Episode: 30 - Steps: 477 - Total Reward: -1.3169999999999886 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.3163000000000166 - Memory Size: 10651\n",
      "Time: 13:06:10 - Episode: 31 - Steps: 230 - Total Reward: 0.4900000000000082 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.32190322580646796 - Memory Size: 10881\n",
      "Time: 13:06:22 - Episode: 32 - Steps: 176 - Total Reward: 1.353999999999989 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.3541562500000155 - Memory Size: 11057\n",
      "Time: 13:06:35 - Episode: 33 - Steps: 241 - Total Reward: -2.3309999999999995 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.27278787878789384 - Memory Size: 11298\n",
      "Time: 13:07:03 - Episode: 34 - Steps: 602 - Total Reward: 2.867999999999969 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.3491176470588372 - Memory Size: 11900\n",
      "Time: 13:07:13 - Episode: 35 - Steps: 139 - Total Reward: -2.7089999999999996 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.26174285714287043 - Memory Size: 12039\n",
      "Time: 13:07:25 - Episode: 36 - Steps: 179 - Total Reward: -0.5989999999999966 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.23783333333334633 - Memory Size: 12218\n",
      "Time: 13:07:49 - Episode: 37 - Steps: 490 - Total Reward: -1.9399999999999964 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.17897297297298573 - Memory Size: 12708\n",
      "Time: 13:07:59 - Episode: 38 - Steps: 117 - Total Reward: -2.767 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.10144736842106507 - Memory Size: 12825\n",
      "Time: 13:08:15 - Episode: 39 - Steps: 309 - Total Reward: -1.2689999999999908 - Best Total Reward: 15.226000000000194 - Average Total Reward: 0.06630769230770465 - Memory Size: 13134\n",
      "Time: 13:08:33 - Episode: 40 - Steps: 334 - Total Reward: -3.8139999999999903 - Best Total Reward: 15.226000000000194 - Average Total Reward: -0.030699999999987716 - Memory Size: 13468\n",
      "Time: 13:08:43 - Episode: 41 - Steps: 167 - Total Reward: -2.6069999999999998 - Best Total Reward: 15.226000000000194 - Average Total Reward: -0.09353658536584167 - Memory Size: 13635\n",
      "Time: 13:09:01 - Episode: 42 - Steps: 348 - Total Reward: -2.6379999999999963 - Best Total Reward: 15.226000000000194 - Average Total Reward: -0.15411904761903583 - Memory Size: 13983\n",
      "Time: 13:09:22 - Episode: 43 - Steps: 463 - Total Reward: -0.3529999999999953 - Best Total Reward: 15.226000000000194 - Average Total Reward: -0.1587441860465 - Memory Size: 14446\n",
      "Time: 13:09:32 - Episode: 44 - Steps: 116 - Total Reward: -2.756 - Best Total Reward: 15.226000000000194 - Average Total Reward: -0.2177727272727159 - Memory Size: 14562\n",
      "Time: 13:09:39 - Episode: 45 - Steps: 71 - Total Reward: -2.911 - Best Total Reward: 15.226000000000194 - Average Total Reward: -0.2776222222222111 - Memory Size: 14633\n",
      "Time: 13:09:52 - Episode: 46 - Steps: 254 - Total Reward: 2.1360000000000032 - Best Total Reward: 15.226000000000194 - Average Total Reward: -0.22515217391303252 - Memory Size: 14887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 13:10:01 - Episode: 47 - Steps: 126 - Total Reward: -2.816 - Best Total Reward: 15.226000000000194 - Average Total Reward: -0.2802765957446702 - Memory Size: 15013\n",
      "Time: 13:10:41 - Episode: 48 - Steps: 952 - Total Reward: -0.7819999999999858 - Best Total Reward: 15.226000000000194 - Average Total Reward: -0.2907291666666559 - Memory Size: 15965\n",
      "Time: 13:11:01 - Episode: 49 - Steps: 379 - Total Reward: 1.9009999999999851 - Best Total Reward: 15.226000000000194 - Average Total Reward: -0.24599999999998975 - Memory Size: 16344\n",
      "Time: 13:11:44 - Episode: 50 - Steps: 1076 - Total Reward: 7.144000000000229 - Best Total Reward: 15.226000000000194 - Average Total Reward: -0.09819999999998537 - Memory Size: 17420\n",
      "Save Loop: 0\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -1.9126914739608765\n",
      "Time: 13:11:59 - Episode: 1 - Steps: 142 - Total Reward: 3.838 - Best Total Reward: 3.838 - Average Total Reward: 3.838 - Memory Size: 17562\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -2.412881851196289\n",
      "Time: 13:12:10 - Episode: 2 - Steps: 110 - Total Reward: -3.08 - Best Total Reward: 3.838 - Average Total Reward: 0.379 - Memory Size: 17672\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -2.9176435470581055\n",
      "Time: 13:12:18 - Episode: 3 - Steps: 49 - Total Reward: -2.999 - Best Total Reward: 3.838 - Average Total Reward: -0.747 - Memory Size: 17721\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -3.204993486404419\n",
      "Time: 13:12:32 - Episode: 4 - Steps: 177 - Total Reward: -2.207 - Best Total Reward: 3.838 - Average Total Reward: -1.112 - Memory Size: 17898\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -3.6363027095794678\n",
      "Time: 13:12:41 - Episode: 5 - Steps: 61 - Total Reward: -2.891 - Best Total Reward: 3.838 - Average Total Reward: -1.4678 - Memory Size: 17959\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -3.844156265258789\n",
      "Time: 13:13:02 - Episode: 6 - Steps: 338 - Total Reward: 1.5719999999999983 - Best Total Reward: 3.838 - Average Total Reward: -0.9611666666666671 - Memory Size: 18297\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -4.198526859283447\n",
      "Time: 13:13:15 - Episode: 7 - Steps: 167 - Total Reward: -2.537 - Best Total Reward: 3.838 - Average Total Reward: -1.1862857142857146 - Memory Size: 18464\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -4.305322170257568\n",
      "Time: 13:14:04 - Episode: 8 - Steps: 986 - Total Reward: 7.464000000000317 - Best Total Reward: 7.464000000000317 - Average Total Reward: -0.10499999999996068 - Memory Size: 19450\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -4.4827799797058105\n",
      "Time: 13:14:17 - Episode: 9 - Steps: 115 - Total Reward: -2.8850000000000002 - Best Total Reward: 7.464000000000317 - Average Total Reward: -0.41388888888885395 - Memory Size: 19565\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -4.546041011810303\n",
      "Time: 13:14:36 - Episode: 10 - Steps: 300 - Total Reward: 5.98000000000001 - Best Total Reward: 7.464000000000317 - Average Total Reward: 0.22550000000003245 - Memory Size: 19865\n",
      "Save Loop: 1\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -4.709780693054199\n",
      "Time: 13:14:48 - Episode: 1 - Steps: 86 - Total Reward: -2.916 - Best Total Reward: -2.916 - Average Total Reward: -2.916 - Memory Size: 19951\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -4.838214874267578\n",
      "Time: 13:15:08 - Episode: 2 - Steps: 287 - Total Reward: -1.8169999999999993 - Best Total Reward: -1.8169999999999993 - Average Total Reward: -2.3664999999999994 - Memory Size: 20238\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -4.852487564086914\n",
      "Time: 13:15:32 - Episode: 3 - Steps: 392 - Total Reward: 3.847999999999978 - Best Total Reward: 3.847999999999978 - Average Total Reward: -0.29500000000000687 - Memory Size: 20630\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -4.918298244476318\n",
      "Time: 13:15:46 - Episode: 4 - Steps: 183 - Total Reward: -2.1729999999999974 - Best Total Reward: 3.847999999999978 - Average Total Reward: -0.7645000000000045 - Memory Size: 20813\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -4.9628825187683105\n",
      "Time: 13:16:05 - Episode: 5 - Steps: 254 - Total Reward: 1.1959999999999948 - Best Total Reward: 3.847999999999978 - Average Total Reward: -0.3724000000000046 - Memory Size: 21067\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -4.953986644744873\n",
      "Time: 13:16:20 - Episode: 6 - Steps: 187 - Total Reward: -2.707 - Best Total Reward: 3.847999999999978 - Average Total Reward: -0.7615000000000038 - Memory Size: 21254\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.000227451324463\n",
      "Time: 13:16:34 - Episode: 7 - Steps: 149 - Total Reward: -2.549 - Best Total Reward: 3.847999999999978 - Average Total Reward: -1.0168571428571462 - Memory Size: 21403\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.053126335144043\n",
      "Time: 13:16:47 - Episode: 8 - Steps: 135 - Total Reward: -2.765 - Best Total Reward: 3.847999999999978 - Average Total Reward: -1.235375000000003 - Memory Size: 21538\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.08527946472168\n",
      "Time: 13:16:59 - Episode: 9 - Steps: 103 - Total Reward: -2.853 - Best Total Reward: 3.847999999999978 - Average Total Reward: -1.4151111111111137 - Memory Size: 21641\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -4.963787078857422\n",
      "Time: 13:17:21 - Episode: 10 - Steps: 378 - Total Reward: -1.2579999999999938 - Best Total Reward: 3.847999999999978 - Average Total Reward: -1.3994000000000018 - Memory Size: 22019\n",
      "Save Loop: 2\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.049386024475098\n",
      "Time: 13:17:41 - Episode: 1 - Steps: 245 - Total Reward: -1.3749999999999951 - Best Total Reward: -1.3749999999999951 - Average Total Reward: -1.3749999999999951 - Memory Size: 22264\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.080603122711182\n",
      "Time: 13:18:03 - Episode: 2 - Steps: 222 - Total Reward: -1.4719999999999944 - Best Total Reward: -1.3749999999999951 - Average Total Reward: -1.4234999999999949 - Memory Size: 22486\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.121403217315674\n",
      "Time: 13:18:15 - Episode: 3 - Steps: 105 - Total Reward: -2.5749999999999997 - Best Total Reward: -1.3749999999999951 - Average Total Reward: -1.80733333333333 - Memory Size: 22591\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.125720977783203\n",
      "Time: 13:18:31 - Episode: 4 - Steps: 190 - Total Reward: -2.3199999999999994 - Best Total Reward: -1.3749999999999951 - Average Total Reward: -1.9354999999999973 - Memory Size: 22781\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.073109149932861\n",
      "Time: 13:19:04 - Episode: 5 - Steps: 558 - Total Reward: -1.427999999999984 - Best Total Reward: -1.3749999999999951 - Average Total Reward: -1.8339999999999947 - Memory Size: 23339\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.086101055145264\n",
      "Time: 13:19:24 - Episode: 6 - Steps: 221 - Total Reward: -2.3409999999999993 - Best Total Reward: -1.3749999999999951 - Average Total Reward: -1.9184999999999954 - Memory Size: 23560\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.124021530151367\n",
      "Time: 13:19:50 - Episode: 7 - Steps: 367 - Total Reward: 1.1329999999999951 - Best Total Reward: 1.1329999999999951 - Average Total Reward: -1.4825714285714253 - Memory Size: 23927\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.127974987030029\n",
      "Time: 13:20:03 - Episode: 8 - Steps: 107 - Total Reward: -2.8770000000000002 - Best Total Reward: 1.1329999999999951 - Average Total Reward: -1.6568749999999972 - Memory Size: 24034\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.064111232757568\n",
      "Time: 13:20:17 - Episode: 9 - Steps: 118 - Total Reward: -2.7779999999999996 - Best Total Reward: 1.1329999999999951 - Average Total Reward: -1.7814444444444417 - Memory Size: 24152\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.097471237182617\n",
      "Time: 13:20:48 - Episode: 10 - Steps: 468 - Total Reward: 0.21199999999999797 - Best Total Reward: 1.1329999999999951 - Average Total Reward: -1.5820999999999978 - Memory Size: 24620\n",
      "Save Loop: 3\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.163569927215576\n",
      "Time: 13:21:03 - Episode: 1 - Steps: 130 - Total Reward: -0.549999999999998 - Best Total Reward: -0.549999999999998 - Average Total Reward: -0.549999999999998 - Memory Size: 24750\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.145386219024658\n",
      "Time: 13:21:19 - Episode: 2 - Steps: 166 - Total Reward: -2.5059999999999993 - Best Total Reward: -0.549999999999998 - Average Total Reward: -1.5279999999999987 - Memory Size: 24916\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800/12800 - loss: -5.172921180725098\n",
      "Time: 13:21:33 - Episode: 3 - Steps: 119 - Total Reward: -2.629 - Best Total Reward: -0.549999999999998 - Average Total Reward: -1.894999999999999 - Memory Size: 25035\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.200456619262695\n",
      "Time: 13:21:46 - Episode: 4 - Steps: 93 - Total Reward: -2.923 - Best Total Reward: -0.549999999999998 - Average Total Reward: -2.1519999999999992 - Memory Size: 25128\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.261957168579102\n",
      "Time: 13:22:01 - Episode: 5 - Steps: 129 - Total Reward: -2.7489999999999997 - Best Total Reward: -0.549999999999998 - Average Total Reward: -2.271399999999999 - Memory Size: 25257\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.273199081420898\n",
      "Time: 13:22:11 - Episode: 6 - Steps: 39 - Total Reward: 4.061000000000001 - Best Total Reward: 4.061000000000001 - Average Total Reward: -1.215999999999999 - Memory Size: 25296\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.252521991729736\n",
      "Time: 13:22:24 - Episode: 7 - Steps: 102 - Total Reward: -2.762 - Best Total Reward: 4.061000000000001 - Average Total Reward: -1.4368571428571422 - Memory Size: 25398\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.22110652923584\n",
      "Time: 13:22:47 - Episode: 8 - Steps: 294 - Total Reward: 3.7159999999999855 - Best Total Reward: 4.061000000000001 - Average Total Reward: -0.7927500000000012 - Memory Size: 25692\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.268062114715576\n",
      "Time: 13:23:01 - Episode: 9 - Steps: 108 - Total Reward: -2.788 - Best Total Reward: 4.061000000000001 - Average Total Reward: -1.0144444444444456 - Memory Size: 25800\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.2445597648620605\n",
      "Time: 13:23:35 - Episode: 10 - Steps: 493 - Total Reward: -0.7530000000000046 - Best Total Reward: 4.061000000000001 - Average Total Reward: -0.9883000000000013 - Memory Size: 26293\n",
      "Save Loop: 4\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.242985725402832\n",
      "Time: 13:23:49 - Episode: 1 - Steps: 92 - Total Reward: -2.822 - Best Total Reward: -2.822 - Average Total Reward: -2.822 - Memory Size: 26385\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.230653762817383\n",
      "Time: 13:24:09 - Episode: 2 - Steps: 224 - Total Reward: 0.7460000000000053 - Best Total Reward: 0.7460000000000053 - Average Total Reward: -1.0379999999999974 - Memory Size: 26609\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.218433380126953\n",
      "Time: 13:24:37 - Episode: 3 - Steps: 391 - Total Reward: 9.099000000000016 - Best Total Reward: 9.099000000000016 - Average Total Reward: 2.341000000000007 - Memory Size: 27000\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.272946834564209\n",
      "Time: 13:24:57 - Episode: 4 - Steps: 226 - Total Reward: -3.285999999999999 - Best Total Reward: 9.099000000000016 - Average Total Reward: 0.9342500000000055 - Memory Size: 27226\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.113886833190918\n",
      "Time: 13:25:50 - Episode: 5 - Steps: 874 - Total Reward: 2.7259999999998983 - Best Total Reward: 9.099000000000016 - Average Total Reward: 1.292599999999984 - Memory Size: 28100\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.14146614074707\n",
      "Time: 13:26:03 - Episode: 6 - Steps: 76 - Total Reward: -3.356 - Best Total Reward: 9.099000000000016 - Average Total Reward: 0.51783333333332 - Memory Size: 28176\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.204685688018799\n",
      "Time: 13:26:22 - Episode: 7 - Steps: 207 - Total Reward: -4.596999999999996 - Best Total Reward: 9.099000000000016 - Average Total Reward: -0.21285714285715368 - Memory Size: 28383\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.172814846038818\n",
      "Time: 13:26:38 - Episode: 8 - Steps: 124 - Total Reward: -2.3839999999999995 - Best Total Reward: 9.099000000000016 - Average Total Reward: -0.4842500000000094 - Memory Size: 28507\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.232645034790039\n",
      "Time: 13:26:55 - Episode: 9 - Steps: 159 - Total Reward: -2.5389999999999997 - Best Total Reward: 9.099000000000016 - Average Total Reward: -0.7125555555555638 - Memory Size: 28666\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.225050449371338\n",
      "Time: 13:27:25 - Episode: 10 - Steps: 415 - Total Reward: -0.5249999999999919 - Best Total Reward: 9.099000000000016 - Average Total Reward: -0.6938000000000066 - Memory Size: 29081\n",
      "Save Loop: 5\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.256564140319824\n",
      "Time: 13:27:41 - Episode: 1 - Steps: 111 - Total Reward: -3.080999999999999 - Best Total Reward: -3.080999999999999 - Average Total Reward: -3.080999999999999 - Memory Size: 29192\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.242193222045898\n",
      "Time: 13:28:04 - Episode: 2 - Steps: 262 - Total Reward: -0.13199999999999346 - Best Total Reward: -0.13199999999999346 - Average Total Reward: -1.6064999999999963 - Memory Size: 29454\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.331782341003418\n",
      "Time: 13:28:18 - Episode: 3 - Steps: 74 - Total Reward: -0.9439999999999973 - Best Total Reward: -0.13199999999999346 - Average Total Reward: -1.385666666666663 - Memory Size: 29528\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.3376784324646\n",
      "Time: 13:28:34 - Episode: 4 - Steps: 112 - Total Reward: -2.562 - Best Total Reward: -0.13199999999999346 - Average Total Reward: -1.6797499999999972 - Memory Size: 29640\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.374087333679199\n",
      "Time: 13:28:47 - Episode: 5 - Steps: 66 - Total Reward: -2.6259999999999994 - Best Total Reward: -0.13199999999999346 - Average Total Reward: -1.8689999999999976 - Memory Size: 29706\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.349737167358398\n",
      "Time: 13:29:05 - Episode: 6 - Steps: 171 - Total Reward: -2.5109999999999997 - Best Total Reward: -0.13199999999999346 - Average Total Reward: -1.975999999999998 - Memory Size: 29877\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.31634521484375\n",
      "Time: 13:29:18 - Episode: 7 - Steps: 62 - Total Reward: -2.9219999999999997 - Best Total Reward: -0.13199999999999346 - Average Total Reward: -2.1111428571428554 - Memory Size: 29939\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.291399002075195\n",
      "Time: 13:29:30 - Episode: 8 - Steps: 62 - Total Reward: 6.278000000000002 - Best Total Reward: 6.278000000000002 - Average Total Reward: -1.0624999999999982 - Memory Size: 30001\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.321314811706543\n",
      "Time: 13:29:44 - Episode: 9 - Steps: 74 - Total Reward: -2.984 - Best Total Reward: 6.278000000000002 - Average Total Reward: -1.2759999999999985 - Memory Size: 30075\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.317819595336914\n",
      "Time: 13:30:02 - Episode: 10 - Steps: 162 - Total Reward: -1.031999999999997 - Best Total Reward: 6.278000000000002 - Average Total Reward: -1.2515999999999983 - Memory Size: 30237\n",
      "Save Loop: 6\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.355836868286133\n",
      "Time: 13:30:22 - Episode: 1 - Steps: 167 - Total Reward: -2.6769999999999996 - Best Total Reward: -2.6769999999999996 - Average Total Reward: -2.6769999999999996 - Memory Size: 30404\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.302612781524658\n",
      "Time: 13:30:45 - Episode: 2 - Steps: 272 - Total Reward: -0.6819999999999931 - Best Total Reward: -0.6819999999999931 - Average Total Reward: -1.6794999999999964 - Memory Size: 30676\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.302628040313721\n",
      "Time: 13:31:06 - Episode: 3 - Steps: 211 - Total Reward: 0.6490000000000031 - Best Total Reward: 0.6490000000000031 - Average Total Reward: -0.9033333333333299 - Memory Size: 30887\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.334514617919922\n",
      "Time: 13:31:21 - Episode: 4 - Steps: 125 - Total Reward: -0.565 - Best Total Reward: 0.6490000000000031 - Average Total Reward: -0.8187499999999974 - Memory Size: 31012\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.294774055480957\n",
      "Time: 13:31:36 - Episode: 5 - Steps: 90 - Total Reward: -2.8699999999999997 - Best Total Reward: 0.6490000000000031 - Average Total Reward: -1.2289999999999979 - Memory Size: 31102\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.340181827545166\n",
      "Time: 13:31:51 - Episode: 6 - Steps: 104 - Total Reward: -2.894 - Best Total Reward: 0.6490000000000031 - Average Total Reward: -1.5064999999999982 - Memory Size: 31206\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.338665962219238\n",
      "Time: 13:32:04 - Episode: 7 - Steps: 72 - Total Reward: -2.912 - Best Total Reward: 0.6490000000000031 - Average Total Reward: -1.7072857142857127 - Memory Size: 31278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.381250381469727\n",
      "Time: 13:32:17 - Episode: 8 - Steps: 77 - Total Reward: -2.827 - Best Total Reward: 0.6490000000000031 - Average Total Reward: -1.8472499999999987 - Memory Size: 31355\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.311717987060547\n",
      "Time: 13:32:49 - Episode: 9 - Steps: 328 - Total Reward: -2.3279999999999994 - Best Total Reward: 0.6490000000000031 - Average Total Reward: -1.9006666666666652 - Memory Size: 31683\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.3541035652160645\n",
      "Time: 13:33:06 - Episode: 10 - Steps: 135 - Total Reward: -2.625 - Best Total Reward: 0.6490000000000031 - Average Total Reward: -1.9730999999999987 - Memory Size: 31818\n",
      "Save Loop: 7\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.372714042663574\n",
      "Time: 13:33:23 - Episode: 1 - Steps: 143 - Total Reward: -2.0230000000000006 - Best Total Reward: -2.0230000000000006 - Average Total Reward: -2.0230000000000006 - Memory Size: 31961\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.319133758544922\n",
      "Time: 13:33:38 - Episode: 2 - Steps: 128 - Total Reward: -2.6079999999999997 - Best Total Reward: -2.0230000000000006 - Average Total Reward: -2.3155 - Memory Size: 32089\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.289644718170166\n",
      "Time: 13:34:13 - Episode: 3 - Steps: 521 - Total Reward: 3.569000000000003 - Best Total Reward: 3.569000000000003 - Average Total Reward: -0.35399999999999904 - Memory Size: 32610\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.25063943862915\n",
      "Time: 13:34:28 - Episode: 4 - Steps: 114 - Total Reward: -2.6439999999999997 - Best Total Reward: 3.569000000000003 - Average Total Reward: -0.9264999999999992 - Memory Size: 32724\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.3450727462768555\n",
      "Time: 13:34:43 - Episode: 5 - Steps: 100 - Total Reward: -2.71 - Best Total Reward: 3.569000000000003 - Average Total Reward: -1.2831999999999995 - Memory Size: 32824\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.2143964767456055\n",
      "Time: 13:35:15 - Episode: 6 - Steps: 448 - Total Reward: 5.362000000000011 - Best Total Reward: 5.362000000000011 - Average Total Reward: -0.17566666666666433 - Memory Size: 33272\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.27468204498291\n",
      "Time: 13:35:37 - Episode: 7 - Steps: 272 - Total Reward: -2.9019999999999997 - Best Total Reward: 5.362000000000011 - Average Total Reward: -0.5651428571428551 - Memory Size: 33544\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.268199443817139\n",
      "Time: 13:36:01 - Episode: 8 - Steps: 279 - Total Reward: -2.9489999999999963 - Best Total Reward: 5.362000000000011 - Average Total Reward: -0.8631249999999977 - Memory Size: 33823\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.255153656005859\n",
      "Time: 13:36:20 - Episode: 9 - Steps: 160 - Total Reward: 3.7800000000000042 - Best Total Reward: 5.362000000000011 - Average Total Reward: -0.3472222222222197 - Memory Size: 33983\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.338435173034668\n",
      "Time: 13:36:36 - Episode: 10 - Steps: 106 - Total Reward: -2.926 - Best Total Reward: 5.362000000000011 - Average Total Reward: -0.6050999999999978 - Memory Size: 34089\n",
      "Save Loop: 8\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.2524895668029785\n",
      "Time: 13:37:07 - Episode: 1 - Steps: 391 - Total Reward: 1.6089999999999782 - Best Total Reward: 1.6089999999999782 - Average Total Reward: 1.6089999999999782 - Memory Size: 34480\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.2870073318481445\n",
      "Time: 13:37:22 - Episode: 2 - Steps: 130 - Total Reward: -2.7899999999999996 - Best Total Reward: 1.6089999999999782 - Average Total Reward: -0.5905000000000107 - Memory Size: 34610\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.280447006225586\n",
      "Time: 13:37:43 - Episode: 3 - Steps: 217 - Total Reward: -2.3969999999999994 - Best Total Reward: 1.6089999999999782 - Average Total Reward: -1.1926666666666736 - Memory Size: 34827\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.336910247802734\n",
      "Time: 13:37:58 - Episode: 4 - Steps: 80 - Total Reward: -2.8 - Best Total Reward: 1.6089999999999782 - Average Total Reward: -1.5945000000000051 - Memory Size: 34907\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.38524055480957\n",
      "Time: 13:38:14 - Episode: 5 - Steps: 108 - Total Reward: -2.768 - Best Total Reward: 1.6089999999999782 - Average Total Reward: -1.8292000000000042 - Memory Size: 35015\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.32064962387085\n",
      "Time: 13:38:34 - Episode: 6 - Steps: 170 - Total Reward: -2.0999999999999996 - Best Total Reward: 1.6089999999999782 - Average Total Reward: -1.8743333333333367 - Memory Size: 35185\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.279514312744141\n",
      "Time: 13:38:56 - Episode: 7 - Steps: 213 - Total Reward: 1.506999999999993 - Best Total Reward: 1.6089999999999782 - Average Total Reward: -1.3912857142857182 - Memory Size: 35398\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.330363750457764\n",
      "Time: 13:39:27 - Episode: 8 - Steps: 393 - Total Reward: -0.8329999999999842 - Best Total Reward: 1.6089999999999782 - Average Total Reward: -1.3215000000000015 - Memory Size: 35791\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.297674179077148\n",
      "Time: 13:39:50 - Episode: 9 - Steps: 227 - Total Reward: -0.2069999999999883 - Best Total Reward: 1.6089999999999782 - Average Total Reward: -1.1976666666666667 - Memory Size: 36018\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.259790420532227\n",
      "Time: 13:40:06 - Episode: 10 - Steps: 111 - Total Reward: 5.269000000000002 - Best Total Reward: 5.269000000000002 - Average Total Reward: -0.5509999999999998 - Memory Size: 36129\n",
      "Save Loop: 9\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.2738566398620605\n",
      "Time: 13:40:31 - Episode: 1 - Steps: 267 - Total Reward: 0.5230000000000055 - Best Total Reward: 0.5230000000000055 - Average Total Reward: 0.5230000000000055 - Memory Size: 36396\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.340469837188721\n",
      "Time: 13:40:47 - Episode: 2 - Steps: 102 - Total Reward: -2.752 - Best Total Reward: 0.5230000000000055 - Average Total Reward: -1.1144999999999972 - Memory Size: 36498\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.244662284851074\n",
      "Time: 13:41:15 - Episode: 3 - Steps: 340 - Total Reward: 4.919999999999991 - Best Total Reward: 4.919999999999991 - Average Total Reward: 0.8969999999999989 - Memory Size: 36838\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.1241655349731445\n",
      "Time: 13:41:50 - Episode: 4 - Steps: 495 - Total Reward: 2.664999999999973 - Best Total Reward: 4.919999999999991 - Average Total Reward: 1.3389999999999924 - Memory Size: 37333\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.224628925323486\n",
      "Time: 13:42:07 - Episode: 5 - Steps: 105 - Total Reward: -2.805 - Best Total Reward: 4.919999999999991 - Average Total Reward: 0.5101999999999939 - Memory Size: 37438\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.199755668640137\n",
      "Time: 13:42:17 - Episode: 6 - Steps: 30 - Total Reward: 4.209999999999999 - Best Total Reward: 4.919999999999991 - Average Total Reward: 1.1268333333333282 - Memory Size: 37468\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.321817398071289\n",
      "Time: 13:42:43 - Episode: 7 - Steps: 295 - Total Reward: 0.6450000000000053 - Best Total Reward: 4.919999999999991 - Average Total Reward: 1.0579999999999963 - Memory Size: 37763\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.282712936401367\n",
      "Time: 13:43:09 - Episode: 8 - Steps: 275 - Total Reward: -4.874999999999994 - Best Total Reward: 4.919999999999991 - Average Total Reward: 0.3163749999999975 - Memory Size: 38038\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.330333709716797\n",
      "Time: 13:43:29 - Episode: 9 - Steps: 141 - Total Reward: -1.6909999999999994 - Best Total Reward: 4.919999999999991 - Average Total Reward: 0.0933333333333312 - Memory Size: 38179\n",
      "Repeat 1/1\n",
      "Epoch 1/1\n",
      "12800/12800 - loss: -5.263906478881836\n",
      "Time: 13:43:57 - Episode: 10 - Steps: 297 - Total Reward: 4.202999999999987 - Best Total Reward: 4.919999999999991 - Average Total Reward: 0.5042999999999968 - Memory Size: 38476\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'saves/'\n",
    "\n",
    "sleep(2)\n",
    "agent.set_playing_data(memorizing=True, verbose=True)\n",
    "env.play_episodes(agent, 50, max_steps, random=True,\n",
    "                  verbose=True, episode_verbose=False)\n",
    "agent.save(save_dir, note='PG_random')\n",
    "\n",
    "agent.set_playing_data(\n",
    "    training=True, memorizing=True, batch_size=64,\n",
    "    mini_batch=12800, epochs=1, repeat=1,\n",
    "    entropy_coef=0, verbose=True\n",
    ")\n",
    "num_episodes = 10\n",
    "for ndx in range(10):\n",
    "    print(f'Save Loop: {ndx}')\n",
    "    result = env.play_episodes(\n",
    "        agent, num_episodes, max_steps,\n",
    "        verbose=True, episode_verbose=False\n",
    "    )\n",
    "    agent.save(save_dir, note=f'PG_{ndx}_{result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 19:15:27 - Episode: 1 - Steps: 627 - Total Reward: 4.6929999999999765 - Best Total Reward: 4.6929999999999765 - Average Total Reward: 4.6929999999999765 - Memory Size: 0\n",
      "Time: 19:15:40 - Episode: 2 - Steps: 163 - Total Reward: -0.6129999999999955 - Best Total Reward: 4.6929999999999765 - Average Total Reward: 2.0399999999999903 - Memory Size: 0\n",
      "Time: 19:16:10 - Episode: 3 - Steps: 587 - Total Reward: 2.5930000000000093 - Best Total Reward: 4.6929999999999765 - Average Total Reward: 2.22433333333333 - Memory Size: 0\n",
      "Time: 19:16:23 - Episode: 4 - Steps: 181 - Total Reward: -1.570999999999997 - Best Total Reward: 4.6929999999999765 - Average Total Reward: 1.275499999999998 - Memory Size: 0\n",
      "Time: 19:16:38 - Episode: 5 - Steps: 258 - Total Reward: -2.3579999999999997 - Best Total Reward: 4.6929999999999765 - Average Total Reward: 0.5487999999999985 - Memory Size: 0\n",
      "Time: 19:16:45 - Episode: 6 - Steps: 59 - Total Reward: 4.091000000000001 - Best Total Reward: 4.6929999999999765 - Average Total Reward: 1.1391666666666656 - Memory Size: 0\n",
      "Time: 19:16:55 - Episode: 7 - Steps: 151 - Total Reward: -2.5409999999999995 - Best Total Reward: 4.6929999999999765 - Average Total Reward: 0.6134285714285707 - Memory Size: 0\n",
      "Time: 19:17:04 - Episode: 8 - Steps: 110 - Total Reward: -2.7399999999999998 - Best Total Reward: 4.6929999999999765 - Average Total Reward: 0.1942499999999993 - Memory Size: 0\n",
      "Time: 19:17:17 - Episode: 9 - Steps: 165 - Total Reward: -1.614999999999995 - Best Total Reward: 4.6929999999999765 - Average Total Reward: -0.006777777777777845 - Memory Size: 0\n",
      "Time: 19:17:27 - Episode: 10 - Steps: 156 - Total Reward: -2.4859999999999998 - Best Total Reward: 4.6929999999999765 - Average Total Reward: -0.25470000000000004 - Memory Size: 0\n",
      "Time: 19:17:40 - Episode: 11 - Steps: 207 - Total Reward: -0.5169999999999937 - Best Total Reward: 4.6929999999999765 - Average Total Reward: -0.27854545454545404 - Memory Size: 0\n",
      "Time: 19:17:56 - Episode: 12 - Steps: 281 - Total Reward: -0.3709999999999951 - Best Total Reward: 4.6929999999999765 - Average Total Reward: -0.2862499999999991 - Memory Size: 0\n",
      "Time: 19:18:22 - Episode: 13 - Steps: 485 - Total Reward: 4.874999999999998 - Best Total Reward: 4.874999999999998 - Average Total Reward: 0.11076923076923145 - Memory Size: 0\n",
      "Time: 19:18:38 - Episode: 14 - Steps: 284 - Total Reward: -1.0039999999999938 - Best Total Reward: 4.874999999999998 - Average Total Reward: 0.031142857142858218 - Memory Size: 0\n",
      "Time: 19:19:12 - Episode: 15 - Steps: 666 - Total Reward: -7.756000000000007 - Best Total Reward: 4.874999999999998 - Average Total Reward: -0.4879999999999995 - Memory Size: 0\n",
      "Time: 19:19:26 - Episode: 16 - Steps: 239 - Total Reward: -0.37899999999999556 - Best Total Reward: 4.874999999999998 - Average Total Reward: -0.4811874999999992 - Memory Size: 0\n",
      "Time: 19:19:51 - Episode: 17 - Steps: 487 - Total Reward: -0.006999999999979245 - Best Total Reward: 4.874999999999998 - Average Total Reward: -0.45329411764705685 - Memory Size: 0\n",
      "Time: 19:20:01 - Episode: 18 - Steps: 143 - Total Reward: 4.337 - Best Total Reward: 4.874999999999998 - Average Total Reward: -0.18716666666666482 - Memory Size: 0\n",
      "Time: 19:20:16 - Episode: 19 - Steps: 239 - Total Reward: 0.2610000000000039 - Best Total Reward: 4.874999999999998 - Average Total Reward: -0.16357894736841913 - Memory Size: 0\n",
      "Time: 19:20:41 - Episode: 20 - Steps: 484 - Total Reward: -1.723999999999985 - Best Total Reward: 4.874999999999998 - Average Total Reward: -0.24159999999999743 - Memory Size: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.24159999999999743"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'saves/trial3'\n",
    "agent.load(path, load_data=False)\n",
    "sleep(2)\n",
    "agent.set_playing_data(training=False, memorizing=False)\n",
    "env.play_episodes(agent, 20, 10000, verbose=True,\n",
    "                  episode_verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
